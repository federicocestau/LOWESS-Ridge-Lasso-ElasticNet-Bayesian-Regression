# LOWESS-Ridge-Lasso-ElasticNet-Bayesian-Regression.
Linear Regression Variations: 
1) The Ordinary Least Squares (OLS) method is one of the most commonly used techniques for regression analysis.
Ordinary least squares (OLS) is a linear regression technique used to find the best-fitting line for a set of data points by minimizing the residuals (the differences between the observed and predicted values). It does so by estimating the coefficients of a linear regression model by minimizing the sum of the squared differences between the observed values of the dependent variable and the predicted values from the model. It is a popular method because it is easy to use and produces decent results. 
2) LOWESS Regression: Simple linear regression has only one slope parameter meaning that it has the same steepness of the curve throughout. Meanwhile, LOWESS can adjust the curve's steepness at various points, producing a better fit than that of simple linear regression. LOWESS is not something that you may want to use in all of your regression models as it follows a non-parametric approach and is quite computationally intensive. However, it is a good way to model a relationship between two variables that do not fit a predefined distribution and have a non-linear relationship.
3) Ridge Regression: Regularization method for linear regression. L2-norm. Ridge regression shrinks the regression coefficients, so that variables (predictors), with minor contribution to the outcome, have their coefficients close to zero. The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical.
When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero. The larger the lambda, the smaller the slope of the fitted line will be. 
One important advantage of the ridge regression, is that it still performs well, compared to the ordinary least square method, in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n).
4) Lasso Regression: It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients. In the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model.
As in ridge regression, selecting a good value of λ for the lasso is critical.
5) Elstic Net Regression: Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO). Therefore, Elastic Net is a combination of both Lasso and Ridge regression. Confusingly, the alpha hyperparameter can be set via the “l1_ratio” argument that controls the contribution of the L1 and L2 penalties and the lambda hyperparameter can be set via the “alpha” argument that controls the contribution of the sum of both penalties to the loss function. 

All regression models assume that independent variables are not correlated that means that a variation in one independent variable does not affect the other independent variables values.
However, it is preferable for regression models that independent variables and the target, dependent variable (the one to make predictions), have high correlation.



